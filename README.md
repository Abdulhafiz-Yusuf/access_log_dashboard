# ğŸ§¾ User Access Log Analysis & Security Monitoring Dashboard  
*(Docker + Airflow + dbt + PostgreSQL + SQLAlchemy + Faker)*

This project demonstrates a **complete, containerized data pipeline** for analyzing user access and login behavior.  
It combines **Python (SQLAlchemy)**, **PostgreSQL**, **dbt**, **Airflow**, and **Power BI**, all running in **Docker**.  
The synthetic log data is generated automatically using the **Faker** library.

[![Access Log Dashboard](/Resources/images/AccessLog_Thumbnail.png)]()

> âš ï¸ All data is **synthetic**, created purely for demonstration and learning purposes.

---

## ğŸ‘¨ğŸ½â€ğŸ’» Author  
**[Abdulhafiz Yusuf](https://github.com/Abdulhafiz-Yusuf)**  
Data Engineering & Analytics Enthusiast  
ğŸ“ Nigeria | ğŸ“ M.Sc. Information Technology (NOUN)

---

## ğŸ“š Table of Contents
- [ğŸ¯ Project Objective](#-project-objective)
- [ğŸ§± Architecture Overview](#-architecture-overview)
- [ğŸ—‚ï¸ Data Design](#-data-design)
- [ğŸ§° Data Generation Script](#-data-generation-script)
- [âš™ï¸ Pipeline Components](#ï¸-pipeline-components)
- [ğŸ‹ Docker Setup](#-docker-setup)
- [ğŸª¶ Airflow DAG Workflow](#-airflow-dag-workflow)
- [ğŸ“Š Power BI Dashboard](#-power-bi-dashboard)
- [ğŸš€ Run the Project](#-run-the-project)
- [ğŸ“ Repository Structure](#-repository-structure)
- [ğŸ’¬ Talking Points](#-talking-points)

---

## ğŸ¯ Project Objective
Analyze **system login and access logs** to detect:
- Repeated failed logins  
- Off-hour access attempts  
- Multiple logins from distinct IP addresses  

This project showcases the **security analytics** capabilities of a modern data engineering stack.

---

## ğŸ§± Architecture Overview
```text
[generate_access_logs.py] â†’ [Python + SQLAlchemy] â†’ [PostgreSQL] â†’ [dbt Models] â†’ [Airflow DAG] â†’ [Power BI Dashboard]
````

| Layer          | Tool           | Purpose                               |
| -------------- | -------------- | ------------------------------------- |
| Data Source    | Python + Faker | Generate synthetic log data           |
| Ingestion      | SQLAlchemy     | Load CSV into PostgreSQL              |
| Storage        | PostgreSQL     | Persist structured log data           |
| Transformation | dbt            | Derive aggregates and flags           |
| Orchestration  | Airflow        | Automate ETL and transformations      |
| Visualization  | Power BI       | Display user activity & anomalies     |
| Deployment     | Docker         | Run everything in isolated containers |

---

## ğŸ—‚ï¸ Data Design

**File:** `/data/access_logs.csv`

| Column      | Example                      | Description             |
| ----------- | ---------------------------- | ----------------------- |
| log_id      | 1                            | Unique log entry ID     |
| user_id     | 112                          | Employee or system user |
| username    | ayusuf                       | User name               |
| login_time  | 2025-10-21 08:15:30          | Login attempt time      |
| logout_time | 2025-10-21 16:30:00          | Logout time             |
| ip_address  | 102.89.44.12                 | Source IP               |
| device_type | Desktop / Mobile             | Device used             |
| status      | SUCCESS / FAILED             | Login status            |
| branch_name | Gusau                        | Branch/office           |
| role        | Teller / Ops / IT / Security | User role category      |

---

## ğŸ§° Data Generation Script

**File:** `generate_access_logs.py`

This script creates a **realistic synthetic log dataset (500â€“1000 rows)** using the `Faker` library.

### ğŸ“„ Key Features:

* Random users, IPs, branches, and roles
* 92% successful vs 8% failed logins
* Realistic work hours (8 AM â€“ 6 PM)
* Automatic logout timestamps for successful sessions

### ğŸ§  Code Snippet:

```python
# generate_access_logs.py
# ------------------------------------------------------------
# Hands-on: Create realistic access_logs.csv (500â€“1000 rows)
# ------------------------------------------------------------
import csv, random
from datetime import datetime, timedelta
from faker import Faker

NUM_ROWS = 750
SEED = 42
OUTPUT_FILE = "access_logs.csv"

random.seed(SEED)
fake = Faker()
Faker.seed(SEED)

USERNAMES = [fake.user_name() for _ in range(120)]
BRANCHES = ["Gusau", "Kano", "Lagos", "Abuja", "Port Harcourt", "Ibadan"]
ROLES = ["Teller", "Ops", "IT", "Security"]
DEVICE_TYPES = ["Desktop", "Mobile"]
STATUSES = ["SUCCESS", "FAILED"]

def random_workday_timestamp(start_hour=8, end_hour=18):
    start = datetime(2025, 1, 1)
    end = datetime(2025, 12, 31)
    random_day = start + timedelta(days=random.randint(0, (end - start).days))
    hour, minute, second = random.randint(start_hour, end_hour - 1), random.randint(0, 59), random.randint(0, 59)
    return random_day.replace(hour=hour, minute=minute, second=second, microsecond=0)

rows = []
for log_id in range(1, NUM_ROWS + 1):
    username = random.choice(USERNAMES)
    user_id = USERNAMES.index(username) + 100
    login_time = random_workday_timestamp()
    status = random.choices(STATUSES, weights=[0.92, 0.08])[0]
    logout_time = login_time + timedelta(hours=random.randint(1, 8)) if status == "SUCCESS" else ""

    rows.append({
        "log_id": log_id,
        "user_id": user_id,
        "username": username,
        "login_time": login_time.strftime("%Y-%m-%d %H:%M:%S"),
        "logout_time": logout_time.strftime("%Y-%m-%d %H:%M:%S") if logout_time else "",
        "ip_address": fake.ipv4(),
        "device_type": random.choice(DEVICE_TYPES),
        "status": status,
        "branch_name": random.choice(BRANCHES),
        "role": random.choice(ROLES),
    })

with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
    writer.writeheader()
    writer.writerows(rows)

print(f"âœ… Created {OUTPUT_FILE} with {len(rows)} rows.")
```

âœ… Output: `access_logs.csv` (750 rows)

---

## âš™ï¸ Pipeline Components

### ğŸ§® Python ETL (SQLAlchemy)

* Creates database & table (`access_logs`)
* Loads CSV data into PostgreSQL
* Runs inside Docker via Airflow

### ğŸ§± dbt Models

* `stg_access_logs.sql` â†’ clean & normalize raw data
* `agg_user_activity.sql` â†’ summarize login behavior
* `suspicious_activity.sql` â†’ flag off-hour and multi-IP users

### ğŸª¶ Airflow DAG

* `access_log_refresh_dag.py`

  * Task 1: Run Python loader
  * Task 2: Execute dbt models
* Scheduled daily (`@daily`) or on demand.

---

## ğŸ‹ Docker Setup

Uses prebuilt images you already have:

| Service       | Image                     | Purpose                        |
| ------------- | ------------------------- | ------------------------------ |
| PostgreSQL    | `postgres:15`             | Database                       |
| Airflow + dbt | `airflow-with-dbt:latest` | Orchestration & transformation |
| Adminer       | `adminer:latest`          | Web database viewer            |

```bash
# Run everything
docker compose up -d
```

Access:

* Airflow â†’ [http://localhost:8080](http://localhost:8080) (admin/admin)
* Adminer â†’ [http://localhost:8081](http://localhost:8081)

---

## ğŸª¶ Airflow DAG Workflow

1ï¸âƒ£ **Load CSV into PostgreSQL**

```python
PythonOperator(
    task_id="load_access_logs",
    python_callable=run_sqlalchemy_loader
)
```

2ï¸âƒ£ **Run dbt Models**

```bash
dbt run --project-dir /opt/airflow/dbt
```

---

## ğŸ“Š Power BI Dashboard

| Page                    | Insights                                |
| ----------------------- | --------------------------------------- |
| **Access Overview**     | Total logins, failed rate, device usage |
| **User Activity**       | Success vs failure by user/role         |
| **Suspicious Behavior** | Off-hour & multi-IP logins by branch    |

**Connect Power BI:**

```
Host: localhost
Port: 5432
Database: airflow
Username: airflow
Password: airflow
```

---

## ğŸš€ Run the Project

### Step-by-Step

```bash
# ğŸ§­ HOW TO RUN
# ------------------------------------------------------------
# 1ï¸âƒ£ Create directories
#     mkdir -p dags logs plugins dbt_project data
#
# 2ï¸âƒ£ Generate fake CSV
#     python3 generate_access_logs.py
#
# 3ï¸âƒ£ Initialize Airflow
#     docker compose up airflow-init
#
# 4ï¸âƒ£ Start the full stack
#     docker compose up -d
#
# 5ï¸âƒ£ Access:
#     Airflow UI â†’ http://localhost:8081
#     Adminer UI â†’ http://localhost:8083
#
# Login:
#     Airflow: admin / admin
#     Adminer: Server=postgres, User=${POSTGRES_USER}, Pass=${POSTGRES_PASSWORD}
# ------------------------------------------------------------
```

---

## ğŸ“ Repository Structure

```
access_log_analysis/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ access_logs.csv
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ access_log_refresh_dag.py
â”‚   â””â”€â”€ load_access_logs_sqlalchemy.py
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ stg_access_logs.sql
â”‚       â”œâ”€â”€ agg_user_activity.sql
â”‚       â””â”€â”€ suspicious_activity.sql
â”œâ”€â”€ generate_access_logs.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ .env.template
â””â”€â”€ README.md
```

---

## ğŸ’¬ Talking Points (for Internal Interview)

> â€œI built a **Dockerized security monitoring pipeline** that generates synthetic access logs, loads them into PostgreSQL using SQLAlchemy, transforms them with dbt, and automates everything through Airflow.
> Finally, I visualize risky login behavior in Power BI â€” helping identify failed logins, off-hour access, and users with multiple IP addresses.â€

---

## âš¡ Tech Stack

* **Python + SQLAlchemy + Faker** â†’ Data generation & ingestion
* **PostgreSQL 15** â†’ Central data storage
* **dbt** â†’ Transformations & analytics models
* **Airflow** â†’ Scheduling & orchestration
* **Docker Compose** â†’ Unified environment
* **Power BI** â†’ Visualization & KPI dashboard

---

## ğŸ”’ Disclaimer

All datasets are **synthetic and anonymized**.
No real user, customer, or institutional data was used.

---

**ğŸ§  Insight:**
This project merges **data engineering**, **security analytics**, and **ETL automation**, reflecting the same skill set required for a **Data Analyst (Information Security)** role.
